{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "from nltk.corpus import framenet as fn\n",
    "from pytorch_pretrained_bert.file_utils import cached_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_VOCAB_ARCHIVE_MAP = {\n",
    "    'bert-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\",\n",
    "    'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt\",\n",
    "    'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt\",\n",
    "    'bert-large-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt\",\n",
    "    'bert-base-multilingual-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt\",\n",
    "    'bert-base-multilingual-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt\",\n",
    "    'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bios2bio(language):\n",
    "    if language == 'en':\n",
    "        ori_data_path = '../data/fn1.7/original/'        \n",
    "    files = glob.glob(ori_data_path+'*.conll')\n",
    "    for file in files:\n",
    "        new_file = file.replace('/original/', '/')\n",
    "        print(file,'-->', new_file)\n",
    "        with open(file,'r') as f:\n",
    "            d = f.readlines()\n",
    "        new_lines = []\n",
    "        for line in d:\n",
    "            new_line = line\n",
    "            if len(line) > 3:\n",
    "                i = line.strip().split('\\t')\n",
    "\n",
    "                if i[14].startswith('S'):\n",
    "                    newtext = i[14].replace('S-', 'B-')\n",
    "                    i[14] = newtext\n",
    "                new_line = '\\t'.join(i)+'\\n'\n",
    "            new_lines.append(new_line)\n",
    "        \n",
    "        with open(new_file, 'w') as f:\n",
    "            for line in new_lines:\n",
    "                f.write(line)\n",
    "\n",
    "# bios2bio('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_idxdata(language):\n",
    "    if language == 'en':\n",
    "        frdata = fn.frames()\n",
    "        ludata = fn.lus()\n",
    "\n",
    "        lu2idx, frame2idx, fe2idx = {},{},{}\n",
    "#         fe2idx['<PAD>'] = len(fe2idx)\n",
    "        for fr in frdata:\n",
    "            frame = fr.name\n",
    "            if frame not in frame2idx:\n",
    "                frame2idx[frame] = len(frame2idx)\n",
    "        with open('../data/fn1.7_frame2idx.json','w') as f:\n",
    "            json.dump(frame2idx, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        for l in ludata:\n",
    "            lu = l.name\n",
    "            if lu not in lu2idx:\n",
    "                lu2idx[lu] = len(lu2idx)\n",
    "        # manual lu not in FN but FN1.7 dataset\n",
    "        lu2idx['burgeon.v'] = len(lu2idx)\n",
    "        lu2idx['but.c'] = len(lu2idx)\n",
    "        lu2idx['however.adv'] = len(lu2idx)\n",
    "        with open('../data/fn1.7_lu2idx.json','w') as f:\n",
    "            json.dump(lu2idx, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        fes = []\n",
    "        for fr in frdata:\n",
    "            for fe_ori in fr.FE:\n",
    "                fes.append(fe_ori)\n",
    "        fes = list(set(fes))\n",
    "        fes.sort()\n",
    "        fe2idx['<pad>'] = len(fe2idx)\n",
    "        fe2idx['O'] = len(fe2idx)\n",
    "        fe2idx['X'] = len(fe2idx)\n",
    "        \n",
    "        for fe in fes:\n",
    "            b_fe = 'B-'+fe\n",
    "            i_fe = 'I-'+fe\n",
    "            if b_fe not in fe2idx:\n",
    "                fe2idx[b_fe] = len(fe2idx)\n",
    "            if i_fe not in fe2idx:\n",
    "                fe2idx[i_fe] = len(fe2idx)        \n",
    "        with open('../data/fn1.7_fe2idx.json','w') as f:\n",
    "            json.dump(fe2idx, f, ensure_ascii=False, indent=4)\n",
    "gen_idxdata('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_map_data(language):\n",
    "    if language == 'en':\n",
    "        with open('../data/fn1.7_lu2idx.json','r') as f:\n",
    "            lu2idx = json.load(f)\n",
    "        with open('../data/fn1.7_frame2idx.json','r') as f:\n",
    "            frame2idx = json.load(f)\n",
    "        with open('../data/fn1.7_fe2idx.json','r') as f:\n",
    "            fe2idx = json.load(f)\n",
    "            \n",
    "        frdata = fn.frames()\n",
    "        ludata = fn.lus()\n",
    "        lufrmap, frargmap = {},{}\n",
    "        for l in ludata:\n",
    "            lu = l.name\n",
    "            lu_idx = int(lu2idx[lu])\n",
    "            frame_candi = l.frame.name\n",
    "            frame_idx = int(frame2idx[frame_candi])\n",
    "            if not lu_idx in lufrmap:\n",
    "                frame_candis = []\n",
    "            else:\n",
    "                frame_candis = lufrmap[lu_idx]\n",
    "            frame_candis.append(frame_idx)\n",
    "            frame_candis = list(set(frame_candis))\n",
    "            lufrmap[lu_idx] = frame_candis\n",
    "        lufrmap[10462] = [883] # lu: burgeon.v, frame: Progression\n",
    "        lufrmap[10463] = [294] # lu: but.c, frame: Concessive\n",
    "        lufrmap[10464] = [294] # lu: however.adv, frame: Concessive\n",
    "        with open('../data/fn1.7_lufrmap.json','w') as f:\n",
    "            json.dump(lufrmap, f, ensure_ascii=False, indent=4)\n",
    "            \n",
    "        for fr in frdata:\n",
    "            frame = fr.name\n",
    "            frame_idx = int(frame2idx[frame])\n",
    "            if not frame_idx in frargmap:\n",
    "                fe_candis = [0,1,2]\n",
    "            else:\n",
    "                fe_candis = frargmap[frame_idx]\n",
    "            for fe in fr.FE:\n",
    "                b_fe_idx = fe2idx['B-'+fe]\n",
    "                i_fe_idx = fe2idx['I-'+fe]\n",
    "                fe_candis.append(b_fe_idx)\n",
    "                fe_candis.append(i_fe_idx)\n",
    "            frargmap[frame_idx] = fe_candis\n",
    "        with open('../data/fn1.7_frargmap.json','w') as f:\n",
    "            json.dump(frargmap, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "gen_map_data('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_lufr_token(language):\n",
    "    if language == 'en':\n",
    "        with open('../data/fn1.7_lu2idx.json','r') as f:\n",
    "            lu2idx = json.load(f)\n",
    "        with open('../data/fn1.7_frame2idx.json','r') as f:\n",
    "            frame2idx = json.load(f)\n",
    "        with open('../data/fn1.7_fe2idx.json','r') as f:\n",
    "            fe2idx = json.load(f)\n",
    "        with open('../data/fn1.7_lufrmap.json','r') as f:\n",
    "            lufrmap = json.load(f)\n",
    "        idx2frame = dict(zip(frame2idx.values(),frame2idx.keys()))\n",
    "        idx2lu = dict(zip(lu2idx.values(),lu2idx.keys()))\n",
    "        idx2fe = dict(zip(fe2idx.values(),fe2idx.keys()))   \n",
    "        \n",
    "        lufr_tokens = []\n",
    "        for i in lufrmap:\n",
    "            luidx = int(i)\n",
    "            fr_candis_idx = lufrmap[str(i)]\n",
    "            lu = idx2lu[luidx]\n",
    "            fr_candis = [idx2frame[fr] for fr in fr_candis_idx]\n",
    "            for fr in fr_candis:\n",
    "                lufr_token = lu+'.'+fr\n",
    "                lufr_token = lufr_token.replace(' ', '_')\n",
    "                lufr_token = '['+lufr_token+']'\n",
    "                lufr_tokens.append(lufr_token)\n",
    "        with open('../data/fn1.7_lufr_tokens','w') as f:\n",
    "            json.dump(lufr_tokens, f, ensure_ascii=False, indent=4)\n",
    "            \n",
    "        ori_vocab_file = PRETRAINED_VOCAB_ARCHIVE_MAP['bert-base-multilingual-cased']\n",
    "        ori_vocab_cache_file = cached_path(ori_vocab_file)\n",
    "        with open(ori_vocab_cache_file,'r') as f:\n",
    "            ori_vocab = f.readlines()\n",
    "        new_vocab = ori_vocab\n",
    "        for i in lufr_tokens:\n",
    "            new_vocab.append(i+'\\n')\n",
    "        with open('../data/fn1.7_lufr_vocab.txt','w') as f:\n",
    "            for i in new_vocab:\n",
    "                f.write(i)\n",
    "            \n",
    "gen_lufr_token('en')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
