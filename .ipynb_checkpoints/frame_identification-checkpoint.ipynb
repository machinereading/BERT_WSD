{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig, BertModel\n",
    "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam\n",
    "from tqdm import tqdm, trange\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)\n",
    "\n",
    "\n",
    "from src import dataio\n",
    "from src.wsd_modeling import BertForWSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# basic settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256 # for English\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### loading en FrameNet data...\n",
      "\t# of sentence in training data: 19391\n",
      "\t# of sentence in dev data: 2272\n",
      "\t# of sentence in test data: 6714\n",
      "\n",
      "data example\n",
      "[['Your', 'contribution', 'to', 'Goodwill', 'will', 'mean', 'more', 'than', 'you', 'may', 'know', '.'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', 'may.v', '_', '_'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', 'Likelihood', '_', '_']]\n"
     ]
    }
   ],
   "source": [
    "language = 'en'\n",
    "fn_version = '1.7'\n",
    "\n",
    "# load framenet 1.7 dataset\n",
    "trn, dev, tst = dataio.load_framenet_data(language)\n",
    "print('\\nan example of dataset')\n",
    "print(trn[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Statistics...\n",
      "\t# of lu: 10464\n",
      "\t# of frame: 1220\n"
     ]
    }
   ],
   "source": [
    "if language == 'en':\n",
    "    data_path = './data/fn'+fn_version+'_'\n",
    "    with open(data_path+'lu2idx.json','r') as f:\n",
    "        lu2idx = json.load(f)\n",
    "    with open(data_path+'frame2idx.json','r') as f:\n",
    "        sense2idx = json.load(f)      \n",
    "    with open(data_path+'lufrmap.json','r') as f:\n",
    "        lusensemap = json.load(f)\n",
    "\n",
    "idx2sense = dict(zip(sense2idx.values(),sense2idx.keys()))\n",
    "idx2lu = dict(zip(lu2idx.values(),lu2idx.keys()))\n",
    "        \n",
    "print('\\nData Statistics...')\n",
    "print('\\t# of lu:', len(lu2idx)-1)\n",
    "print('\\t# of frame:', len(sense2idx)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lu2idx['eat.v']: 3101\n",
      "lusensemap['3101']: [609]\n",
      "sense2idx['Ingestion']: 609\n"
     ]
    }
   ],
   "source": [
    "# example of 2idx\n",
    "print('\\nexamples of idx data')\n",
    "print(\"lu2idx['eat.v']:\", lu2idx['eat.v'])\n",
    "print(\"lusensemap['3101']:\", lusensemap['3101'])\n",
    "print(\"sense2idx['Ingestion']:\", sense2idx['Ingestion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load BERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "\n",
    "# bert tokenizer\n",
    "def bert_tokenizer(text):\n",
    "    orig_tokens = text.split(' ')\n",
    "    bert_tokens = []\n",
    "    orig_to_tok_map = []\n",
    "    bert_tokens.append(\"[CLS]\")\n",
    "    for orig_token in orig_tokens:\n",
    "        orig_to_tok_map.append(len(bert_tokens))\n",
    "        bert_tokens.extend(tokenizer.tokenize(orig_token))\n",
    "    bert_tokens.append(\"[SEP]\")\n",
    "    \n",
    "    return orig_tokens, bert_tokens, orig_to_tok_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gen BERT input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(input_data):\n",
    "    tokenized_texts, lus, senses = [],[],[]\n",
    "\n",
    "    for i in range(len(input_data)):    \n",
    "        data = input_data[i]\n",
    "        text = ' '.join(data[0])\n",
    "        orig_tokens, bert_tokens, orig_to_tok_map = bert_tokenizer(text)\n",
    "        tokenized_texts.append(bert_tokens)\n",
    "\n",
    "        ori_lus = data[1]    \n",
    "        lu_sequence = []\n",
    "        for i in range(len(bert_tokens)):\n",
    "            if i in orig_to_tok_map:\n",
    "                idx = orig_to_tok_map.index(i)\n",
    "                l = ori_lus[idx]\n",
    "                lu_sequence.append(l)\n",
    "            else:\n",
    "                lu_sequence.append('_')\n",
    "        lus.append(lu_sequence)        \n",
    "        \n",
    "        ori_senses = data[2]    \n",
    "        sense_sequence = []\n",
    "        for i in range(len(bert_tokens)):\n",
    "            if i in orig_to_tok_map:\n",
    "                idx = orig_to_tok_map.index(i)\n",
    "                l = ori_senses[idx]\n",
    "                sense_sequence.append(l)\n",
    "            else:\n",
    "                sense_sequence.append('_')\n",
    "        senses.append(sense_sequence)\n",
    "\n",
    "    input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "    \n",
    "    tgt_seq, lu_seq, sense_seq = [],[],[]\n",
    "    for sent_idx in range(len(lus)):\n",
    "        lu_items = lus[sent_idx]\n",
    "        sense_items = senses[sent_idx]\n",
    "        tgt,lu, sense = [],[],[]\n",
    "        for idx in range(len(lu_items)):\n",
    "            if lu_items[idx] != '_':\n",
    "                if len(tgt) == 0:\n",
    "                    tgt.append(idx)\n",
    "                    lu.append(lu2idx[lu_items[idx]])\n",
    "        for idx in range(len(sense_items)):\n",
    "            if sense_items[idx] != '_':\n",
    "                if len(sense) == 0:\n",
    "                    sense.append(sense2idx[sense_items[idx]])\n",
    "        tgt_seq.append(tgt)\n",
    "        lu_seq.append(lu)\n",
    "        sense_seq.append(sense)\n",
    "        \n",
    "    attention_masks = [[float(i>0) for i in ii] for ii in input_ids]    \n",
    "    \n",
    "    data_inputs = torch.tensor(input_ids)\n",
    "    data_tgt_idx = torch.tensor(tgt_seq)\n",
    "    data_lus = torch.tensor(lu_seq)\n",
    "    data_senses = torch.tensor(sense_seq)\n",
    "    data_masks = torch.tensor(attention_masks)\n",
    "    \n",
    "    return data_inputs, data_tgt_idx, data_lus, data_senses, data_masks\n",
    "\n",
    "trn_inputs, trn_tgt_idxs, trn_lus, trn_senses, trn_masks = gen_data(trn)\n",
    "dev_inputs, dev_tgt_idxs, dev_lus, dev_senses, dev_masks = gen_data(dev)\n",
    "tst_inputs, tst_tgt_idxs, tst_lus, tst_senses, tst_masks = gen_data(tst)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_data = TensorDataset(trn_inputs, trn_tgt_idxs, trn_lus, trn_senses, trn_masks)\n",
    "trn_sampler = RandomSampler(trn_data)\n",
    "trn_dataloader = DataLoader(trn_data, sampler=trn_sampler, batch_size=batch_size)\n",
    "\n",
    "dev_data = TensorDataset(dev_inputs, dev_tgt_idxs, dev_lus, dev_senses, dev_masks)\n",
    "dev_sampler = RandomSampler(dev_data)\n",
    "dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=batch_size)\n",
    "\n",
    "tst_data = TensorDataset(tst_inputs, tst_tgt_idxs, tst_lus, tst_senses, tst_masks)\n",
    "tst_sampler = RandomSampler(tst_data)\n",
    "tst_dataloader = DataLoader(tst_data, sampler=tst_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load BERT_WSD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForWSD.from_pretrained(\"bert-base-multilingual-cased\", num_labels = len(sense2idx), num_lus = len(lu2idx), ludim = 64, lusensemap=lusensemap)\n",
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING the pretrained BERT LM for WSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-38292d8ad11b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-38292d8ad11b>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m                          lus=b_input_lus, senses=b_input_frames, attention_mask=b_input_masks)\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;31m# track train loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def training():    \n",
    "    epochs = 20\n",
    "    max_grad_norm = 1.0\n",
    "    num_of_epoch = 0\n",
    "    accuracy_result = []\n",
    "    for _ in trange(epochs, desc=\"Epoch\"):\n",
    "        # TRAIN loop\n",
    "        model.train()\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        for step, batch in enumerate(trn_dataloader):\n",
    "            # add batch to gpu\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_tgt_idxs, b_input_lus, b_input_senses, b_input_masks = batch            \n",
    "            # forward pass\n",
    "            loss = model(b_input_ids, token_type_ids=None, tgt_idxs=b_input_tgt_idxs, \n",
    "                         lus=b_input_lus, senses=b_input_senses, attention_mask=b_input_masks)\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # track train loss\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "            # gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "        # print train loss per epoch\n",
    "        print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "        num_of_epoch += 1\n",
    "        model_path = './models/frame_identifier-epoch-'+str(num_of_epoch)+'.pt'\n",
    "        torch.save(model, model_path)        \n",
    "\n",
    "        # evaluation for each epoch\n",
    "        model.eval()\n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "        predictions , true_labels, scores, candis, all_lus = [], [], [], [], []\n",
    "        for batch in tst_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_tgt_idxs, b_lus, b_senses, b_masks = batch\n",
    "\n",
    "            with torch.no_grad():\n",
    "                tmp_eval_loss = model(b_input_ids, token_type_ids=None, tgt_idxs=b_tgt_idxs, \n",
    "                                     lus=b_lus, senses=b_senses, attention_mask=b_masks)\n",
    "                logits = model(b_input_ids, token_type_ids=None, tgt_idxs=b_tgt_idxs, \n",
    "                                lus=b_lus, attention_mask=b_masks)\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_senses.to('cpu').numpy()          \n",
    "            masks = dataio.get_masks(b_lus, lusensemap, num_label=len(sense2idx)).to(device)\n",
    "            for lu in b_lus:\n",
    "                candi_idx = lusensemap[str(int(lu))]\n",
    "                candi = [idx2sense[c] for c in candi_idx]\n",
    "                candi_txt = ','.join(candi)\n",
    "                candi_txt = str(len(candi))+'\\t'+candi_txt\n",
    "                candis.append(candi_txt)\n",
    "                all_lus.append(idx2lu[int(lu)])\n",
    "            \n",
    "            for b_idx in range(len(logits)):\n",
    "                logit = logits[b_idx]\n",
    "                mask = masks[b_idx]\n",
    "                b_pred_idxs, b_pred_logits = [],[]\n",
    "                for fr_idx in range(len(mask)):\n",
    "                    if mask[fr_idx] > 0:\n",
    "                        b_pred_idxs.append(fr_idx)\n",
    "                        b_pred_logits.append(logit[0][fr_idx].item())\n",
    "                b_pred_idxs = torch.tensor(b_pred_idxs)\n",
    "                b_pred_logits = torch.tensor(b_pred_logits)\n",
    "                sm = nn.Softmax()\n",
    "                b_pred_logits = sm(b_pred_logits).view(1, -1)\n",
    "                score, indice = b_pred_logits.max(1)                \n",
    "                prediction = b_pred_idxs[indice]\n",
    "                predictions.append([int(prediction)])\n",
    "                score = float(score)\n",
    "                scores.append(score)\n",
    "            true_labels.append(label_ids)\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "            nb_eval_examples += b_input_ids.size(0)\n",
    "            nb_eval_steps += 1\n",
    "            \n",
    "        eval_loss = eval_loss/nb_eval_steps\n",
    "        print(\"Validation loss: {}\".format(eval_loss))\n",
    "        print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "        pred_tags = [idx2sense[p_i] for p in predictions for p_i in p]\n",
    "        valid_tags = [idx2sense[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n",
    "        acc = accuracy_score(pred_tags, valid_tags)\n",
    "        accuracy_result.append(acc)\n",
    "        print(\"Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))\n",
    "        with open('./result/frame_identification/'+str(num_of_epoch)+'.tsv','w') as f:\n",
    "            line = 'gold' + '\\t' + 'prediction' + '\\t' + 'score' + '\\t' + 'input_lu' + '\\t' + 'sense_candidates'\n",
    "            f.write(line+'\\n')\n",
    "            for item in range(len(pred_tags)):\n",
    "                line = valid_tags[item] + '\\t' + pred_tags[item] + '\\t' + str(scores[item]) +'\\t'+ all_lus[item]+'\\t' + candis[item]\n",
    "                f.write(line+'\\n')\n",
    "    with open('./result/frameid.accuracy','w') as f:\n",
    "        n = 0\n",
    "        for acc in accuracy_result:\n",
    "            f.write('epoch:'+str(n)+'\\t' + 'accuracy: '+str(acc)+'\\n')\n",
    "            n +=1\n",
    "\n",
    "training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
